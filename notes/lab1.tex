\documentclass[letterpaper,12pt]{article}
\usepackage{amssymb,latexsym,amsmath}
\usepackage{wasysym}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\begin{document}

\begin{flushleft}
AI/ML Guild Lab \#1\\
Bayesian Classification\\
\today \\
\end{flushleft}

\section{Introduction}

Thomas Bayes: minister, mathematician. Born in London 1701, published ``An Essay
towards solving a Problem in the Doctrine of Chances'' in 1763. It was basically
a treatise on card counting.

\section{Naive Bayes Classification}

Basic Bayesian Statistics
\begin{itemize}
    \item Conditional Probability: The probability of observing some event, $X$,
        given that another event, $Y$ has already been observed. Denoted: $P(X \mid Y)$
    \item Prior probability: A probability distribution, $P(X)$, that expresses one's
        belief in an outcome before any evidence is collected.
    \item Posterior probability: The probability of an event after all the
        predictor information has been incorporated. Posterior probabilities
        reflect the uncertainty of assessing an observation to particular class.
        \[ P(\theta \mid X) = \frac{P(X \mid \theta)P(\theta)}{P(X)} \]
\end{itemize}

\subsection{Bayes Theorem}

\[
    P(\theta \mid X) = \frac{P(X \mid \theta)P(\theta)}{P(X)}
\]

This is just the posterior probability, derived in Bayes' original essay.

$P(\theta)$ is the prior distribution

The likelihood $P(X \mid \theta)$ is the evidence of $\theta$ provided by $X$

$P(X)$ is the probability of $X$ given all possible $\theta$.

\subsection{Naive Bayes}

Exact Bayesian Classification is often impractical, getting the \emph{true}
value of the likelihood may be intractable or impossible to acquire. The
likelihood is defined as $P(x_1, x_2, \dots, x_j \mid \theta)$.

We therefore must approximate $P(X \mid \theta)$ by calculating the product of the
individual conditional probabilities: $P(x_1 \mid \theta)*P(x_2 \mid
\theta)*\dots*P(x_j \mid \theta)$

$P(X)$ is the same for all values, so we don't even need to calculate it, we can
ignore it.

We end up with an approximation of the value we want $P(\theta \mid X)$:

\[
    P(\theta \mid X) \propto P(X \mid \theta)*P(\theta) = P(\theta)*P(x_1 \mid \theta)*P(x_2 \mid \theta)*\dots*P(x_j \mid \theta)
\]


Classificaiton then becomes assigning the label with the highest likelihood
value for a given observation.

\end{document}

